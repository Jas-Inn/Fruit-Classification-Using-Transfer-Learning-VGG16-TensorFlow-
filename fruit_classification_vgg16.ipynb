{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fruit Image Classification Using Transfer Learning (VGG16)\n\nIn this project I built an image classifier that can recognize 24 different fruit (and vegetable) categories from photos. Instead of training a deep neural network from scratch \u2014 which would require a massive dataset and a lot of compute \u2014 I used **transfer learning** with **VGG16**, a well-known convolutional neural network that was pre-trained on ImageNet (1.2 million images, 1000 classes).\n\nThe idea is simple: VGG16 already knows how to detect edges, textures, shapes and patterns. I kept all of that knowledge, chopped off its original classification head, and replaced it with my own layers trained specifically to tell fruits apart.\n\nThe pipeline goes like this:\n1. Download and set up the Fruits-360 dataset\n2. Build data generators with augmentation\n3. Load VGG16 and add custom classification layers on top\n4. Train with the VGG16 weights frozen (feature extraction phase)\n5. Unfreeze the last few VGG16 layers and fine-tune\n6. Evaluate on the test set and visualize predictions\n\nFinal test accuracy: **~86%** across 24 classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n\nFirst I suppress some noisy TensorFlow/Keras warnings that show up when running on CPU, and make sure the environment stays clean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras.src.trainers.data_adapters.py_dataset_adapter\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras.src.trainers.epoch_iterator\")\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I install the specific library versions I used so the notebook is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.16.2 matplotlib==3.9.2 numpy==1.26.4 scipy==1.14.1 scikit-learn==1.5.2 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset \u2014 Fruits-360\n\nI used the **Fruits-360** dataset, which contains high-quality photos of fruits and vegetables on white backgrounds. It's already split into Training, Validation, and Test folders, and each class has its own subdirectory \u2014 which makes it very easy to load with Keras's `flow_from_directory`.\n\nThe version I downloaded has:\n- **24 classes** (mostly apple varieties, plus pear, cucumber, carrot, eggplant, zucchini, etc.)\n- **~12,500 images** total across all three splits\n\nThe code below downloads the zip from the course server, extracts it in chunks (it's large), and cleans up after itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the dataset...\nDownload completed.\nExtracting the dataset in chunks...\nExtracted 2000 of 12481 files...\nExtracted 4000 of 12481 files...\nExtracted 6000 of 12481 files...\nExtracted 8000 of 12481 files...\nExtracted 10000 of 12481 files...\nExtracted 12000 of 12481 files...\nExtracted 12481 of 12481 files...\nDone. Extracted to 'fruits-360-original-size'.\nCleaned up fruits-360-original-size.zip\n"
     ]
    }
   ],
   "source": [
    "import os\nimport subprocess\nimport zipfile\n\nurl = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4yIRGlIpNfKEGJYMhZV52g/fruits-360-original-size.zip\"\nlocal_zip = \"fruits-360-original-size.zip\"\nextract_dir = \"fruits-360-original-size\"\n\ndef download_dataset(url, output_file):\n    print(\"Downloading the dataset...\")\n    subprocess.run([\"wget\", \"-q\", \"-O\", output_file, url], check=True)\n    print(\"Download completed.\")\n\ndef extract_zip_in_chunks(zip_file, extract_to, batch_size=2000):\n    \"\"\"Extract a large zip in batches to avoid memory bottlenecks.\"\"\"\n    print(\"Extracting the dataset in chunks...\")\n    os.makedirs(extract_to, exist_ok=True)\n    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n        files = zip_ref.namelist()\n        total_files = len(files)\n        for i in range(0, total_files, batch_size):\n            for file in files[i:i+batch_size]:\n                zip_ref.extract(file, extract_to)\n            print(f\"Extracted {min(i+batch_size, total_files)} of {total_files} files...\")\n    print(f\"Done. Extracted to '{extract_to}'.\")\n\nif not os.path.exists(local_zip):\n    download_dataset(url, local_zip)\n\nif not os.path.exists(extract_dir):\n    extract_zip_in_chunks(local_zip, extract_dir)\n\nif os.path.exists(local_zip):\n    os.remove(local_zip)\n    print(f\"Cleaned up {local_zip}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Imports and Dataset Paths\n\nNow I import everything I'll need and point the generators at the right directories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\ntrain_dir = 'fruits-360-original-size/fruits-360-original-size/Training'\nval_dir   = 'fruits-360-original-size/fruits-360-original-size/Validation'\ntest_dir  = 'fruits-360-original-size/fruits-360-original-size/Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each directory contains one subfolder per class. Keras will automatically use the folder names as class labels \u2014 no manual labelling needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Generators & Augmentation\n\nI use Keras's `ImageDataGenerator` to load images on the fly in batches, which means I don't have to load the whole dataset into memory at once.\n\nFor the **training set** I also apply data augmentation \u2014 random rotations, flips, zooms, and shifts. This artificially increases the variety of images the model sees during training, which helps it generalize better and not just memorize the training set.\n\nFor **validation and test**, I only rescale pixel values to [0, 1]. No augmentation there \u2014 I want those to reflect real-world performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6231 images belonging to 24 classes.\nFound 3114 images belonging to 24 classes.\nFound 3110 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n    rescale=1.0/255.0,\n    rotation_range=20,\n    width_shift_range=0.1,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen  = ImageDataGenerator(rescale=1.0/255.0)\ntest_datagen = ImageDataGenerator(rescale=1.0/255.0)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir, target_size=(64, 64), batch_size=16, class_mode='categorical'\n)\nval_generator = val_datagen.flow_from_directory(\n    val_dir, target_size=(64, 64), batch_size=16, class_mode='categorical'\n)\ntest_generator = test_datagen.flow_from_directory(\n    test_dir, target_size=(64, 64), batch_size=16, class_mode='categorical'\n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 6,231 training images, 3,114 validation, and 3,110 test images \u2014 all automatically labelled and loaded in batches of 16. Images are resized to 64\u00d764 pixels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Architecture \u2014 VGG16 + Custom Head\n\nHere's the core of transfer learning. I load VGG16 with its ImageNet weights, but with `include_top=False` \u2014 this strips off the original fully-connected classification layers (which were built for 1000 ImageNet classes) so I can replace them with my own.\n\nI then freeze all the VGG16 layers. This means during the first training phase, their weights don't change at all \u2014 they're just used as a fixed feature extractor. Only my custom head gets trained.\n\nMy custom head is:\n- `GlobalAveragePooling2D` \u2014 collapses the spatial dimensions of VGG16's output into a single vector\n- `Dense(256, relu)` \u2014 a fully connected layer to learn fruit-specific patterns\n- `BatchNormalization` \u2014 stabilizes training\n- `Dropout(0.3)` \u2014 randomly drops 30% of neurons each step to reduce overfitting\n- `Dense(24, softmax)` \u2014 the final output layer, one probability per class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n\n# Freeze all VGG16 layers \u2014 we're not touching their weights yet\nfor layer in base_model.layers:\n    layer.trainable = False\n\nmodel = Sequential([\n    base_model,\n    GlobalAveragePooling2D(),\n    Dense(256, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.3),\n    Dense(train_generator.num_classes, activation='softmax')\n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I compile the model. I use `categorical_crossentropy` as the loss function because this is a multi-class problem (24 classes, one correct answer per image). Adam is a solid default optimizer that adapts the learning rate during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n    loss='categorical_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Phase 1 \u2014 Feature Extraction Training\n\nI train for 5 epochs with two callbacks:\n- **ReduceLROnPlateau**: if validation loss stops improving for 2 epochs, the learning rate is cut by 80% to give the optimizer a chance to find a better minimum\n- **EarlyStopping**: if val loss doesn't improve for 5 epochs, training stops early and the best weights are restored \u2014 this prevents wasting time and avoids overfitting\n\nI limit to 50 steps per epoch and 25 validation steps. On a CPU this keeps training to a reasonable time while still making good progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n50/50 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56s 1s/step - accuracy: 0.2675 - loss: 2.6561 - val_accuracy: 0.2100 - val_loss: 2.6586 - learning_rate: 0.0010\nEpoch 2/5\n50/50 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 52s 1s/step - accuracy: 0.5213 - loss: 1.5834 - val_accuracy: 0.4225 - val_loss: 2.1430 - learning_rate: 0.0010\nEpoch 3/5\n50/50 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 52s 1s/step - accuracy: 0.5863 - loss: 1.3203 - val_accuracy: 0.4650 - val_loss: 1.8615 - learning_rate: 0.0010\nEpoch 4/5\n50/50 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 51s 1s/step - accuracy: 0.6587 - loss: 1.0643 - val_accuracy: 0.5375 - val_loss: 1.5257 - learning_rate: 0.0010\nEpoch 5/5\n50/50 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 52s 1s/step - accuracy: 0.6913 - loss: 0.9276 - val_accuracy: 0.6525 - val_loss: 1.2305 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.mixed_precision import set_global_policy\n\nlr_scheduler  = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6, verbose=1)\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\nset_global_policy('float32')\n\nhistory = model.fit(\n    train_generator,\n    epochs=5,\n    validation_data=val_generator,\n    steps_per_epoch=50,\n    validation_steps=25,\n    callbacks=[lr_scheduler, early_stopping]\n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 5 epochs with the VGG16 weights frozen, the model reaches about **69% training accuracy** and **65% validation accuracy**. Not bad \u2014 but we can do better by letting the model adapt some of the lower layers too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Phase 2 \u2014 Fine-Tuning\n\nNow I unfreeze the **last 5 layers** of VGG16 (the deepest convolutional blocks) so the model can start adapting those representations to fruit images specifically.\n\nI keep the BatchNorm layers frozen because updating them with a small dataset can destabilize training.\n\nI recompile with a much lower learning rate (`1e-5` instead of `1e-3`). This is important \u2014 if the learning rate is too high during fine-tuning, we'd risk destroying the pre-trained weights we just spent time using.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 has 19 layers total.\nEpoch 1/5\n50/50 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 73s 1s/step - accuracy: 0.7383 - loss: 0.8548 - val_accuracy: 0.6450 - val_loss: 1.0274 - learning_rate: 1.0000e-05\nEpoch 2/5\n50/50 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 67s 1s/step - accuracy: 0.7613 - loss: 0.7665 - val_accuracy: 0.7275 - val_loss: 0.8596 - learning_rate: 1.0000e-05\nEpoch 3/5\n50/50 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 67s 1s/step - accuracy: 0.8462 - loss: 0.5940 - val_accuracy: 0.7525 - val_loss: 0.7244 - learning_rate: 1.0000e-05\nEpoch 4/5\n50/50 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 68s 1s/step - accuracy: 0.8450 - loss: 0.5283 - val_accuracy: 0.8675 - val_loss: 0.5083 - learning_rate: 1.0000e-05\nEpoch 5/5\n50/50 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 68s 1s/step - accuracy: 0.8537 - loss: 0.5283 - val_accuracy: 0.8250 - val_loss: 0.5381 - learning_rate: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "num_layers = len(base_model.layers)\nprint(f\"VGG16 has {num_layers} layers total.\")\n\n# Unfreeze the last 5 layers\nfor layer in base_model.layers[-5:]:\n    layer.trainable = True\n\n# Keep BatchNorm layers frozen to keep training stable\nfor layer in base_model.layers:\n    if isinstance(layer, tf.keras.layers.BatchNormalization):\n        layer.trainable = False\n\n# Recompile with a low learning rate so we don't overwrite the pre-trained knowledge\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n    metrics=['accuracy']\n)\n\nhistory_fine = model.fit(\n    train_generator,\n    epochs=5,\n    validation_data=val_generator,\n    steps_per_epoch=50,\n    validation_steps=25,\n    callbacks=[lr_scheduler, early_stopping]\n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning made a significant difference. Validation accuracy jumped from ~65% to ~82\u201386% in just 5 more epochs. Unfreezing even a small number of layers lets the model adjust its learned features to be more specific to fruits rather than generic ImageNet patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation on the Test Set\n\nNow I run the model against the held-out test set \u2014 images it has never seen during training or validation. This gives the real measure of how well it generalizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 38s 749ms/step - accuracy: 0.8625 - loss: 0.5041\nTest Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=50)\nprint(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**86% accuracy on 24 classes** \u2014 that's solid, especially given I only trained for 10 epochs total and used a CPU. The gap between training (~85%) and test accuracy (~86%) is negligible, which means the model isn't overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Curves\n\nI plot the accuracy and loss across both training phases to visualize the learning progression. Combining both histories side by side makes it easy to see the effect of fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy curves \u2014 both phases\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'],     label='Train (frozen)')\nplt.plot(history.history['val_accuracy'], label='Val (frozen)')\nplt.plot(history_fine.history['accuracy'],     label='Train (fine-tuned)')\nplt.plot(history_fine.history['val_accuracy'], label='Val (fine-tuned)')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Accuracy over Training')\nplt.legend()\nplt.grid(True)\n\n# Loss curves \u2014 both phases\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'],     label='Train (frozen)')\nplt.plot(history.history['val_loss'], label='Val (frozen)')\nplt.plot(history_fine.history['loss'],     label='Train (fine-tuned)')\nplt.plot(history_fine.history['val_loss'], label='Val (fine-tuned)')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss over Training')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see two distinct learning phases in the curves. The first phase (frozen layers) shows a steady improvement. The second phase (fine-tuning) shows a jump in both accuracy and loss reduction \u2014 that's the model's deeper layers adapting to fruit-specific features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visual Predictions on Sample Test Images\n\nFinally, I pick a few images from the test set and run them through the model to see predictions side by side with the actual labels.\n\nThe function reads the true class directly from the folder name (so no hardcoded labels), preprocesses the image the same way as training, and shows the result visually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\n\nactual_count    = Counter()\npredicted_count = Counter()\n\ndef get_class_name_from_index(predicted_index, class_index_mapping):\n    for class_name, index in class_index_mapping.items():\n        if index == predicted_index:\n            return class_name\n    return \"Unknown\"\n\ndef visualize_prediction(img_path, class_index_mapping):\n    # True label comes from the folder name \u2014 no hardcoding\n    true_class = os.path.basename(os.path.dirname(img_path))\n\n    img = load_img(img_path, target_size=(64, 64))\n    img_array = img_to_array(img) / 255.0\n    img_array = np.expand_dims(img_array, axis=0)\n\n    prediction = model.predict(img_array, verbose=0)\n    predicted_index = np.argmax(prediction, axis=-1)[0]\n    predicted_class = get_class_name_from_index(predicted_index, class_index_mapping)\n\n    actual_count[true_class]        += 1\n    predicted_count[predicted_class] += 1\n\n    plt.figure(figsize=(2, 2), dpi=100)\n    plt.imshow(img)\n    color = 'green' if true_class == predicted_class else 'red'\n    plt.title(f\"True: {true_class}\\nPred: {predicted_class}\", color=color, fontsize=7)\n    plt.axis('off')\n    plt.show()\n\nclass_index_mapping = train_generator.class_indices\n\nsample_images = [\n    'fruits-360-original-size/fruits-360-original-size/Test/apple_braeburn_1/r0_11.jpg',\n    'fruits-360-original-size/fruits-360-original-size/Test/pear_1/r0_103.jpg',\n    'fruits-360-original-size/fruits-360-original-size/Test/cucumber_3/r0_103.jpg',\n]\n\nfor img_path in sample_images:\n    visualize_prediction(img_path, class_index_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The title turns green for correct predictions and red for wrong ones. Misclassifications tend to happen between visually similar classes \u2014 for example, different apple varieties (Braeburn vs Golden vs Red Delicious all look quite alike). This is expected and would improve with more training epochs or higher-resolution images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n\nThis project shows how powerful transfer learning is for image classification tasks, especially when you don't have a huge dataset or a GPU. By reusing VGG16's convolutional features \u2014 which it learned from millions of images \u2014 I got to 86% accuracy on 24 fruit classes in just 10 epochs of training.\n\nThe two-phase approach (freeze \u2192 fine-tune) is a standard and effective strategy: the first phase teaches the custom head to work with VGG16's features, and the second phase lets the deeper layers refine themselves for the specific task.\n\nThings that could improve it further:\n- Training for more epochs with patience\n- Using a higher input resolution (e.g. 128\u00d7128)\n- Trying a more modern backbone like EfficientNetV2 or MobileNetV3\n- Adding more augmentation variety\n"
   ]
  }
 ]
}
